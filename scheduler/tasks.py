import subprocess
import datetime
import sys
import logging
from sqlalchemy import or_

from scheduler.celery_app import celery_app as celery
from scheduler.scheduling import utcnow, next_fire
from database.base import SessionLocal
from database.models.task import JobSchedule, JobRun, ConflictPolicy, Task, RunState


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


def run_module(module: str, task_id: int, timeout: int | None = None):
    cmd = [sys.executable, "-m", module, str(task_id)]
    logger.info(f"–ó–∞–ø—É—Å–∫ –º–æ–¥—É–ª—è {module} –¥–ª—è task_id={task_id}")
    proc = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
    return proc.returncode, proc.stdout, proc.stderr


@celery.task(name="scheduler.scheduler_tick")
def scheduler_tick():
    logger.info("‚ñ∂Ô∏è scheduler_tick –∑–∞–ø—É—â–µ–Ω")
    now = utcnow()
    with SessionLocal() as db:
        due = JobSchedule.filter_ex(
            where=[
                JobSchedule.enabled.is_(True),
                JobSchedule.next_run_at.isnot(None),
                JobSchedule.next_run_at <= now,
                or_(JobSchedule.locked_until.is_(None), JobSchedule.locked_until < now),
            ],
            order_by=JobSchedule.next_run_at.asc(),
            limit=50,
            for_update=True,
            skip_locked=True,
            db=db,
        )

        enqueued = 0
        for s in due:
            active = JobRun.filter_ex(
                where=[JobRun.schedule_id == s.id, JobRun.state == RunState.RUNNING],
                db=db,
            )
            if len(active) >= s.max_instances and s.policy == ConflictPolicy.SKIP:
                logger.info(f"‚è∏ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {s.name}, –∑–∞–Ω—è—Ç–æ {len(active)} –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤")
                s.locked_until = now + datetime.timedelta(seconds=5)
                db.commit()
                continue

            pipeline = (s.kwargs or {}).get("pipeline", [])
            run = JobRun.create(schedule_id=s.id, state=RunState.QUEUED, attempt=0)
            task = Task.create(
                state=RunState.QUEUED,
                step_index=0,
                steps_total=len(pipeline),
                step_name=None,
                pipeline=pipeline,
            )
            run.task_id = task.id

            s.last_run_at = now
            s.next_run_at = next_fire(s, now)
            s.locked_until = None
            db.commit()

            celery.send_task(
                s.task_name or "scheduler.pipeline_execute",
                args=[run.id],
                queue=s.queue or "executor",
            )
            logger.info(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω pipeline {s.name} (run_id={run.id})")
            enqueued += 1

        logger.info(f"‚úÖ scheduler_tick –∑–∞–≤–µ—Ä—à—ë–Ω, –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ {enqueued} –∑–∞–¥–∞—á")
        return enqueued


@celery.task(bind=True, name="scheduler.pipeline_execute")
def pipeline_execute(self, run_id: int):
    logger.info(f"‚ñ∂Ô∏è pipeline_execute –∑–∞–ø—É—â–µ–Ω (run_id={run_id}) –Ω–∞ {self.request.hostname}")
    hostname = self.request.hostname
    with SessionLocal() as db:
        run = JobRun.get(id=run_id)
        if not run:
            logger.warning(f"‚ùå Run {run_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return "missing"

        task = Task.get(id=run.task_id) if run.task_id else None
        if not task:
            sched = JobSchedule.get(id=run.schedule_id)
            pipeline = (sched.kwargs or {}).get("pipeline", []) if sched else []
            task = Task.create(state=RunState.QUEUED, step_index=0, steps_total=len(pipeline), pipeline=pipeline)
            run.task_id = task.id
            run.save()

        pipeline = task.pipeline or []
        total = len(pipeline)

        run.state = RunState.RUNNING
        run.started_at = datetime.datetime.utcnow()
        run.save()

        task.state = RunState.RUNNING
        task.locked_by = hostname
        task.locked_at = datetime.datetime.utcnow()
        task.add_log("pipeline start")

        i = task.step_index or 0
        try:
            while i < total:
                step = pipeline[i]
                name = step.get("name") or f"step_{i+1}"
                module = step.get("module")

                task.step_index = i
                task.steps_total = total
                task.step_name = name
                task.add_log(f"{name} start")
                logger.info(f"‚ñ∂Ô∏è –®–∞–≥ {i+1}/{total}: {name} (–º–æ–¥—É–ª—å={module})")

                code, out, err = run_module(module, task.id)
                if out:
                    task.add_log(out[:2000])
                    logger.info(f"‚ÑπÔ∏è stdout: {out[:200].strip()}...")
                if code != 0:
                    if err:
                        task.add_log(err[:2000])
                        logger.error(f"‚ùå stderr: {err[:200].strip()}...")
                    raise RuntimeError(f"{name} failed with exit {code}")

                task.add_log(f"{name} done")
                task.save()
                logger.info(f"‚úÖ –®–∞–≥ {name} –∑–∞–≤–µ—Ä—à—ë–Ω")
                i += 1

            task.step_index = total
            task.step_name = "finished"
            task.state = RunState.SUCCESS
            task.locked_by = None
            task.locked_at = None
            task.error = None
            task.add_log("pipeline finished")
            task.save()

            run.state = RunState.SUCCESS
            run.finished_at = datetime.datetime.utcnow()
            run.save()
            logger.info(f"üéâ pipeline_execute run_id={run_id} –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
            return "ok"

        except Exception as e:
            task.state = RunState.FAILED
            task.locked_by = None
            task.locked_at = None
            task.error = str(e)
            task.add_log(f"pipeline failed: {e}")
            task.save()

            run.state = RunState.FAILED
            run.error = str(e)
            run.finished_at = datetime.datetime.utcnow()
            run.save()
            logger.error(f"üî• pipeline_execute run_id={run_id} —É–ø–∞–ª: {e}")
            raise


@celery.task(name="scheduler.scheduler_retry")
def scheduler_retry():
    logger.info("‚ñ∂Ô∏è scheduler_retry –∑–∞–ø—É—â–µ–Ω")
    failed = JobRun.filter(state=RunState.FAILED, limit=100)
    restarted = 0
    for run in failed:
        sched = JobSchedule.get(id=run.schedule_id)
        if not sched:
            continue

        if run.attempt >= sched.retry_limit:
            logger.info(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º run_id={run.id}, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ä–µ—Ç—Ä–∞–µ–≤")
            continue

        delay = min(sched.backoff_initial * (2 ** max(run.attempt - 1, 0)), sched.backoff_max)

        run.state = RunState.QUEUED
        run.attempt = (run.attempt or 0) + 1
        run.error = None
        run.started_at = None
        run.finished_at = None
        run.save()

        celery.send_task(
            sched.task_name or "scheduler.pipeline_execute",
            args=[run.id],
            queue=sched.queue or "executor",
            countdown=delay,
        )
        restarted += 1
        logger.info(f"üîÑ run_id={run.id} –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å")

    logger.info(f"‚úÖ scheduler_retry –∑–∞–≤–µ—Ä—à—ë–Ω, –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–æ {restarted} –∑–∞–¥–∞—á")
    return restarted
